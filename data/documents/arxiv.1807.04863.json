{
  "title": "Avoiding Latent Variable Collapse With Generative Skip Models",
  "authors": [
    "Adji B. Dieng",
    "Yoon Kim",
    "Alexander M. Rush",
    "David M. Blei"
  ],
  "abstract": "  Variational autoencoders (VAEs) learn distributions of high-dimensional data.\nThey model data by introducing a deep latent-variable model and then maximizing\na lower bound of the log marginal likelihood. While VAEs can capture complex\ndistributions, they also suffer from an issue known as \"latent variable\ncollapse.\" Specifically, the lower bound involves an approximate posterior of\nthe latent variables; this posterior \"collapses\" when it is set equal to the\nprior, i.e., when the posterior is independent of the data. While VAEs learn\ngood generative models, latent variable collapse prevents them from learning\nuseful representations. In this paper, we propose a new way to avoid latent\nvariable collapse. We expand the model class to one that includes skip\nconnections; these connections enforce strong links between the latent\nvariables and the likelihood function. We study these generative skip models\nboth theoretically and empirically. Theoretically, we prove that skip models\nincrease the mutual information between the observations and the inferred\nlatent variables. Empirically, on both images (MNIST and Omniglot) and text\n(Yahoo), we show that generative skip models lead to less collapse than\nexisting VAE architectures.\n",
  "id": "arxiv.1807.04863",
  "url": "https://arxiv.org/abs/1807.04863",
  "pdf": "https://arxiv.org/pdf/1807.04863",
  "bibtex": "@misc{dieng2018_arxiv:1807.04863,\n    title = {Avoiding Latent Variable Collapse With Generative Skip Models},\n    author = {Adji B. Dieng, Yoon Kim, Alexander M. Rush, David M. Blei},\n    year = {2018},\n    archiveprefix = {arXiv},\n    eprint = {1807.04863},\n    pdf = {https://arxiv.org/pdf/1807.04863},\n    url = {https://arxiv.org/abs/1807.04863}\n}",
  "source": "arxiv.org",
  "date": 1545528496,
  "tags": []
}