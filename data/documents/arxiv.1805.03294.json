{
  "title": "Improved training of end-to-end attention models for speech recognition",
  "authors": [
    "Albert Zeyer",
    "Kazuki Irie",
    "Ralf Schlüter",
    "Hermann Ney"
  ],
  "abstract": "Sequence-to-sequence attention-based models on subword units allow simple\nopen-vocabulary end-to-end speech recognition. In this work, we show that such\nmodels can achieve competitive results on the Switchboard 300h and LibriSpeech\n1000h tasks. In particular, we report the state-of-the-art word error rates\n(WER) of 3.54% on the dev-clean and 3.82% on the test-clean evaluation subsets\nof LibriSpeech. We introduce a new pretraining scheme by starting with a high\ntime reduction factor and lowering it during training, which is crucial both\nfor convergence and final performance. In some experiments, we also use an\nauxiliary CTC loss function to help the convergence. In addition, we train long\nshort-term memory (LSTM) language models on subword units. By shallow fusion,\nwe report up to 27% relative improvements in WER over the attention baseline\nwithout a language model.",
  "id": "arxiv.1805.03294",
  "url": "https://arxiv.org/abs/1805.03294",
  "pdf": "https://arxiv.org/pdf/1805.03294",
  "bibtex": "@misc{zeyer2018_arxiv:1805.03294,\n    title = {Improved training of end-to-end attention models for speech recognition},\n    author = {Albert Zeyer and Kazuki Irie and Ralf Schlüter and Hermann Ney},\n    year = {2018},\n    archiveprefix = {arXiv},\n    eprint = {1805.03294},\n    pdf = {https://arxiv.org/pdf/1805.03294},\n    url = {https://arxiv.org/abs/1805.03294}\n}",
  "source": "arxiv.org",
  "date": 1558082910,
  "tags": [],
  "api": "https://export.arxiv.org/api/query?id_list=1805.03294"
}