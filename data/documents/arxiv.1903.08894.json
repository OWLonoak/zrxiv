{
  "title": "Towards Characterizing Divergence in Deep Q-Learning",
  "authors": [
    "Joshua Achiam",
    "Ethan Knight",
    "Pieter Abbeel"
  ],
  "abstract": "Deep Q-Learning (DQL), a family of temporal difference algorithms for\ncontrol, employs three techniques collectively known as the `deadly triad' in\nreinforcement learning: bootstrapping, off-policy learning, and function\napproximation. Prior work has demonstrated that together these can lead to\ndivergence in Q-learning algorithms, but the conditions under which divergence\noccurs are not well-understood. In this note, we give a simple analysis based\non a linear approximation to the Q-value updates, which we believe provides\ninsight into divergence under the deadly triad. The central point in our\nanalysis is to consider when the leading order approximation to the deep-Q\nupdate is or is not a contraction in the sup norm. Based on this analysis, we\ndevelop an algorithm which permits stable deep Q-learning for continuous\ncontrol without any of the tricks conventionally used (such as target networks,\nadaptive gradient optimizers, or using multiple Q functions). We demonstrate\nthat our algorithm performs above or near state-of-the-art on standard MuJoCo\nbenchmarks from the OpenAI Gym.",
  "id": "arxiv.1903.08894",
  "url": "https://arxiv.org/abs/1903.08894",
  "pdf": "https://arxiv.org/pdf/1903.08894",
  "bibtex": "@misc{achiam2019_arxiv:1903.08894,\n    title = {Towards Characterizing Divergence in Deep Q-Learning},\n    author = {Joshua Achiam and Ethan Knight and Pieter Abbeel},\n    year = {2019},\n    archiveprefix = {arXiv},\n    eprint = {1903.08894},\n    pdf = {{https://arxiv.org/pdf/1903.08894}},\n    url = {https://arxiv.org/abs/1903.08894}\n}",
  "source": "arxiv.org",
  "date": 1553397503,
  "tags": [],
  "api": "https://export.arxiv.org/api/query?id_list=1903.08894"
}